{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41919bc1-30e8-41d9-8956-8916f937fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Como calculam os pesos/erros\n",
    "# AdaBoost: Aumenta os pesos das amostras mal classificadas em cada iteração. O foco é forçar os próximos modelos a corrigirem os erros dos anteriores.\n",
    "\n",
    "# GBM: Modela os resíduos (erros) da predição anterior, ou seja, cada novo modelo tenta prever o erro do modelo anterior.\n",
    "\n",
    "# 2. Função de perda usada\n",
    "# AdaBoost: Usa implicitamente uma função de perda exponencial (por isso é sensível a outliers).\n",
    "\n",
    "# GBM: Permite escolher diversas funções de perda (ex: MSE, MAE, log-loss), tornando-o mais flexível.\n",
    "\n",
    "# 3. Peso dos estimadores\n",
    "# AdaBoost: Cada modelo fraco recebe um peso α com base em sua performance.\n",
    "\n",
    "# GBM: Os modelos são somados com uma taxa de aprendizado, mas não recebem pesos explícitos baseados em erro.\n",
    "\n",
    "# 4. Sensibilidade a outliers\n",
    "#AdaBoost: É mais sensível a outliers, pois o erro exponencial penaliza fortemente erros grandes.\n",
    "\n",
    "# GBM: É menos sensível a outliers dependendo da função de perda usada (ex: MAE é mais robusto).\n",
    "\n",
    "# 5. Implementação e flexibilidade\n",
    "#AdaBoost: Mais simples, com menos opções de customização.\n",
    "#GBM: Mais flexível e poderoso, permite ajuste fino (função de perda, subsample, regularização, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7922dd49-997a-4494-8e86-6fe2a7bc4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eedb2e2-d2ae-491a-ae0e-09ea45d8c60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 1.0\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eecf493b-17dc-4798-86ce-16f731cbc7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5369947659860906\n",
      "R²: 0.7803012822391022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shinoki\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"R²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369b201-eca8-4c1b-9ded-097262c0e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O Stochastic Gradient Boosting introduz uma amostragem aleatória (subamostragem) dos dados em cada iteração do boosting, ou seja, em vez de usar o dataset completo para treinar cada árvore base, ele usa uma amostra aleatória (sem reposição) de uma fração dos dados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
